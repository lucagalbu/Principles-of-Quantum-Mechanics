%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  My documentation report
%  Objetive: Explain what I did and how, so someone can continue with the investigation
%
% Important note:
% Chapter heading images should have a 2:1 width:height ratio,
% e.g. 920px width and 460px height.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[11pt,fleqn]{book} % Default font size and left-justified equations

\usepackage[top=3cm,bottom=3cm,left=3.2cm,right=3.2cm,headsep=10pt,letterpaper]{geometry} % Page margins

\usepackage{xcolor} % Required for specifying colors by name
\definecolor{ocre}{RGB}{52,177,201} % Define the orange color used for highlighting throughout the book

% Font Settings
\usepackage{avant} % Use the Avantgarde font for headings
%\usepackage{times} % Use the Times font for headings
\usepackage{mathptmx} % Use the Adobe Times Roman as the default text font together with math symbols from the Sym­bol, Chancery and Com­puter Modern fonts
\usepackage{microtype} % Slightly tweak font spacing for aesthetics
\usepackage[utf8]{inputenc} % Required for including letters with accents
\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage{amsthm}

% Bibliography
\usepackage{csquotes}
\usepackage[style=alphabetic,sorting=nyt,sortcites=true,autopunct=true,autolang=hyphen,hyperref=true,abbreviate=false,backref=true,backend=biber]{biblatex}
\addbibresource{bibliography.bib} % BibTeX bibliography file
\defbibheading{bibempty}{}

\input{structure} % Insert the commands.tex file which contains the majority of the structure behind the template

%----------------------------------------------------------------------------------------
%	Definitions of new commands
%----------------------------------------------------------------------------------------

\def\R{\mathbb{R}}
\newcommand{\cvx}{convex}
\newcommand{\bra}[1]{\langle #1|}
\newcommand{\ket}[1]{| #1\rangle}
\newcommand{\scalar}[2]{\langle #1| #2\rangle}
\begin{document}

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\begingroup
\thispagestyle{empty}
\AddToShipoutPicture*{\put(0,0){\includegraphics[scale=1.25]{esahubble}}} % Image background
\centering
\vspace*{5cm}
\par\normalfont\fontsize{35}{35}\sffamily\selectfont
\textbf{PRINCIPLES OF QUANTUM MECHANICS}\\
{\LARGE R. Shankar}\par % Book title
\vspace*{1cm}
{\Huge Lecture Notes}\par % Author name
\endgroup

%----------------------------------------------------------------------------------------
%	COPYRIGHT PAGE
%----------------------------------------------------------------------------------------

\newpage
~\vfill
\thispagestyle{empty}

%\noindent Copyright \copyright\ 2014 Andrea Hidalgo\\ % Copyright notice

\noindent \textsc{Notes from the book Principles of Quantum Mechanics.}
\vspace{\baselineskip}

\noindent These notes have been taking while self-studying the book Principles of Quantum Mechanics by R. Shankar, second edition.

\noindent \textit{First release, July 2022} % Printing/edition date

%----------------------------------------------------------------------------------------
%	TABLE OF CONTENTS
%----------------------------------------------------------------------------------------

\chapterimage{head1.png} % Table of contents heading image

\pagestyle{empty} % No headers

\tableofcontents % Print the table of contents itself

%\cleardoublepage % Forces the first chapter to start on an odd page so it's on the right

\pagestyle{fancy} % Print headers again

%----------------------------------------------------------------------------------------
%	CHAPTER 1
%----------------------------------------------------------------------------------------

\chapterimage{head2.png} % Chapter heading image
\chapter{Mathematical introduction}
\section{Vector spaces}
\begin{definition}[Group]
    A group is a set $G$ together with a binary operation here denoted "$\cdot$", that combines any two elements  $a$ and $b$ 
    to form an element of $G$, denoted $a\cdot b$, such that the following three requirements, known as group \textit{axioms}, 
    are satisfied
    \begin{enumerate}
        \item Associativity: $\forall\: a,b,c \in G\; (a\cdot b)\cdot c = a\cdot (b\cdot c)$.
        \item Identity: $\exists\: e \in G\;\mid\; \forall a\in G\; e\cdot a=a\cdot e=a$. 
        \item Inverse $\forall a\in G\;\exists\; a^{-1}\in G \mid a^{-1}\cdot a=a\cdot a^{-1}=e$.
    \end{enumerate}
\end{definition}

\begin{remark}
    In the definition of the identity and inverse element, we clearly stated that they commute with any element of the group. This is
    important to state in the defition, because the operation defined on the group is not necessarily commutative.    
\end{remark}

\begin{remark}
    From the assioms it follows that the identity element is unique. If $e$ and $f$ are both identity elements, then $e=e\cdot f=f$.
    
    Similarly, the inverse of an element $a$ of the group is also unique. If $a$ has both $a_1^{-1}$ and $a_2^{-1}$ as inverse, then
    $a_2^{-1} = a_2^{-1}\cdot e = a_2^{-1}\cdot(a\cdot a_1^{-1}) = (a_2^{-1}\cdot a)\cdot a_1^{-1}=e\cdot a_1^{-1}=a_1^{-1}$.
\end{remark}

\begin{definition}[Abelian group]
    A group is \textit{abelian} if the operation defined on it is commutative.
\end{definition}

\begin{definition}[Field]
    A field is a set $F$ together with two binary operations, called addition and multiplication; 
    it is an \textit{abelian} group under addition with $0$ as the additive identity; 
    the \textit{non zero} elements are an \textit{abelian} group under multiplication with $1$ as the multiplicative identity; 
    and multiplication distributes over addition: $\forall a,b\in F\; a\cdot(b+c)=(a\cdot b) + (a\cdot c)$.
\end{definition}

\begin{remark}
    $\R^2$ is a field with the multiplication defined by $(a,b)\cdot(c,d)=(ac-bd, ad+bc)$. This is exactly the multiplication of two
    complex numbers. Indeed, the complex numbers is the way to give $\R^2$ a field structure. For $n>2$ it is not possible to give $\R^n$
    the structure of a field. However, for $n=4$ we can have a pseudo field using the non-commutative quaternion multiplication.
    For $\R^8$ we can give up the associativity of the multiplication and get the non-associative Caley algebra (octanions).
\end{remark}

\begin{definition}[Vector space]
A vector space over a field $F$ is a set $V$ together with two binary operations. 
The first operation, called vector addition assigns to any two vectors $\ket{V}$ and $\ket{W}$ in $V$ a third vector in $V$ which is commonly written as $\ket{v+w}$.
The second operation, called scalar multiplication, assigns to any scalar $a$ in $F$ and any vector $\ket{V}$ in $V$ another vector in $V$, which is denoted $a\ket{V}$.
These two operations must satisfy the eight following axioms:
\begin{enumerate}
    \item Associativity of vector addition:
    $\forall\: \ket{U}, \ket{V}, \ket{W} \in V\; \ket{U}+(\ket{V}+\ket{W}) = (\ket{U}+\ket{V})+\ket{W}$.
    \item Commutativity of vector addition: $\forall\: \ket{U}, \ket{V} \in V\; \ket{U}+\ket{V}=\ket{V}+\ket{U}$.
    \item Identity element of vector addition:
    $\exists\: \ket{0}\in V\;\mid\;\forall \ket{V}\in V\;\ket{V} + \ket{0}= \ket{V}$. 
    \item Inverse elements of vector addition: $\forall \ket{V}\in V\;\exists\; \ket{-V}\in V \mid 
    \ket{V}+\ket{-V}=\ket{0}$.
    \item Compatibility of scalar multiplication with field multiplication:\\
    $\forall\: a,b\in F\: \ket{V}\in V\;a(b\ket{V}) = (ab)\ket{V}$.
    \item Identity element of scalar multiplication: if $1$ is the multiplicative identity in $F$, then $\forall\:\ket{V}\in V\;1\ket{V}=\ket{V}$.
    \item Distributivity of scalar multiplication with respect to vector addition: $\forall\: a\in F\:\ket{U},\ket{V}\in V\: a(\ket{U}+\ket{V})=a\ket{U}+a\ket{V}$.
    \item Distributivity of scalar multiplication with respect to field addition: 
    $\forall\: a,b\in F\:\ket{V}\in V\:(a+b)\ket{V} = a\ket{V}+b\ket{V}$.
\end{enumerate}
Subtraction of two vectors can be defined as $\ket{V}-\ket{W} = \ket{V} + \ket{-w}$.
\end{definition}

\begin{remark}
    From the axioms it follows that 
    \begin{enumerate}
        \item $\ket{0}$ and the inverse are unique. The proof is the same as for the case of the group above.
        \item $0\ket{V}=0$: $\ket{V}=1\ket{V}=(0+1)\ket{V}=0\ket{V}+\ket{V}\Rightarrow 0\ket{V}=0$.
        \item $\ket{-V}=-\ket{V}$
    \end{enumerate}
\end{remark}

\begin{example}
    Consider all functions $f(x)$ defined in an interval $0<x<L$ and such that $f(0)=f(L)=0$. 
    We define scalar multiplication by $a$ simply as $af(x)$ and addition as pointwise addition. These functions form a vector space where 
    $\ket{0}$ is the function which is zero everywhere and the additive inverse of $\ket{f(x)}$ is $\ket{-f(x)}$.
    Notice that if $f(0)=f(L)\neq 0$ then we do not have a vector space.
\end{example}

\subsection{Linear independency}
\begin{definition}[Linear independency]
    A set of vectors $\ket{i}$ is said to be linearly independent if
    $\sum a_i\ket{i}=\ket{0}$
    is satisfied only by the trivial solution $a_i=0$.
\end{definition}

\begin{remark}
    If a set of vectors is linearly dependent, there must exist at least two $a_i\neq 0$. Let's say $a_3\neq 0$, then
    $\ket{3}=-\sum_{i\neq 3}\frac{a_i}{a_3}\ket{i}$. That is, in a set of linearly dependent vectors, at least one of them
    can be written as a linear combination of the other. On the other hand, it is not possible to write any member of the
    linearly independent set in terms of the others.
\end{remark}

\begin{definition}[Dimension of the vector space]
    A vector space has dimension $n$ if it can accommodate a maximum
    of $n$ linearly independent vectors. It will be denoted by $V^n$.
\end{definition}

\begin{example}
    The set of $2 \times 2$ matrices is a four-dimensional vector space. In fact, the following four vectors are linearly indpenedent:
    \begin{equation*}
        \begin{array}{cccc}
            \ket{1}=\left[
                \begin{array}{cc}
                    1&0\\
                    0&0
                \end{array}
        \right] &
        \ket{2}=\left[
            \begin{array}{cc}
                0&1\\
                0&0
            \end{array}
        \right] &
        \ket{3}=\left[
            \begin{array}{cc}
                0&0\\
                1&0
            \end{array}
        \right] &
        \ket{4}=\left[
            \begin{array}{cc}
                0&0\\
                0&1
            \end{array}
        \right]
        \end{array}
    \end{equation*} 
    Notice that any other matrix can be written as a linear combination of these four matrices. We have seen that in a set of linearly
    independent vectors, no vector can be written as a linear combination of the others. This means that the four matrices given above 
    represent the largest set of linearly independent vectors, and the vector space of $2\times 2$ matrices has dimension $n=4$.
\end{example}

\begin{definition}[Basis]
    A set of $n$ linearly independent vectors in an $n$-dimensional space is called a \textit{basis}.
\end{definition}

\begin{definition}[Component]
    Given a basis $\ket{i}$, any vector $\ket{V}$ can be written as a linear combination of the vectors $\ket{i}$. The coefficients of the
    linear combination are called the \textit{components} of $\ket{V}$ in the basis $\ket{i}$.
\end{definition}

\begin{theorem}
    The expansion of a vector in a given basis is unique.
\end{theorem}
\begin{proof}
    Let's suppose that there exist two different expansions:
    \begin{equation*}
        \ket{V}=\sum a_i\ket{i} = \sum a'_i\ket{i}
    \end{equation*}
    It follows
    \begin{equation*}
        \sum a_i\ket{i} - \sum a'_i\ket{i} =  \sum (a_i-a'_i)\ket{i} = \ket{0}
    \end{equation*}
    Since $\ket{i}$ are linearly independent, this means that $a_i=a'_i$ and the expansion is unique.
\end{proof}

\subsection{Inner product space}
\begin{definition}[Inner product]
    The \textit{inner product} or \textit{scalar product} is an operation that takes two vectors $\ket{V}$ and $\ket{W}$ and
    returns a scalar, denoted $\scalar{W}{V}$ satisfying the following axioms:
    \begin{enumerate}
        \item Skew symmetry: $\scalar{W}{V}=\scalar{V}{W}^*$.
        \item Positive semidefiniteness: $\scalar{V}{V}\geq 0$ and the equality holds iff $\ket{V}=\ket{0}$.
        \item Linearity in the ket: $\bra{W}(a\ket{V}+b\ket{Z})=a\scalar{W}{V}+b\scalar{W}{Z}\equiv\scalar{W}{aV+bZ}$.
    \end{enumerate}
    A vector space together with an inner product is called \textit{inner product space}.
\end{definition}

\begin{remark}
    From the axioms of the inner product it follows:
    \begin{enumerate}
        \item $\scalar{V}{V}$ is a real non-negative number. Therefore, $\sqrt{\scalar{V}{V}}$ is well defined.
        \item Antilinearity with respect to the first factor: $\scalar{aW+bZ}{V}=a^*\scalar{W}{V}+b^*\scalar{Z}{V}$. 
    \end{enumerate}
\end{remark}

\begin{definition}[Norm and orthogonality]
    The \textit{norm} of a vector is defined as $|V|\equiv\sqrt{\scalar{V}{V}}$. 
    Two vectors $\ket{V}$ and $\ket{W}$ are \textit{orthogonal} if $\scalar{W}{V}=0$.
\end{definition}

\begin{remark}
    Orthogonal vectors are also linear independent. If $\ket{V}$ and $\ket{W}$ are orthogonal, let's consider
    $a\ket{V}+b\ket{W}=0$. Taking the inner product with $\ket{V}$ and using the orthogonality, we get $a=0$. Similarly, the
    inner product with $\ket{W}$ leads to $b=0$.
\end{remark}

\begin{definition}[Dual space and bras]
    Any ket $\ket{V}$ induces a linear function $\scalar{V}{\cdot}$ that associates to each ket $\ket{W}$ its inner product with 
    $\ket{V}$. The space of all linear functions $\scalar{V}{\cdot}$ generated by all vectors $\ket{V}$ of a vector space $V$ is
    is itself a vector space and is called the \textit{dual} of $V$. 
    The elements of the dual space are called \textit{bras} and denoted by $\bra{V}$.
\end{definition}

An inner product can take the form of any function satisfying the axioms. However, given an orthonormal basis, the inner product can be
expressed in an easy way in terms of the components of the vectors. Let $\ket{i}$ be an orthonormal basis, and let 
$\ket{V}=\sum v_i\ket{i}$ and $\ket{W}=\sum w_i\ket{i}$. Then, 
$\scalar{W}{V}=\sum_{i,j}w^*_iv_j\scalar{i}{j}=\sum_{i,j}w^*_iv_j\delta_{ij}=\sum_i w^*_iv_i$.
I.e., in an orthonormal basis, the inner product is just the sum of the products of the single components. However, which basis is 
orthonormal depends on the specific inner product chosen.

Notice that it is convention to write the components of a ket as a column vector.
So the inner product is a number we are trying to generate from two kets $\ket{V}$ and $\ket{W}$, which are both represented by column vectors
in some basis. Now there is no way to make a number out of two columns by direct matrix multiplication, but there is a way to make a number 
by matrix multiplication of a row times a column. In particular, if we represent the bra $\bra{W}$ with the \textit{adjoint} (transpose
conjugate) of $\ket{W}$, in an orthonormal basis we have 
\begin{equation}
    \scalar{W}{V}=[w^*_1,...,w^*_n]
    \left[\begin{array}{c}
        v^1\\
        \vdots\\
        v^n
    \end{array}\right]=
    \sum_i w^*_iv^i\equiv w^*_iv^i
\end{equation}

\begin{remark}
    The components of the kets are written with lower indexes, while the components of a bra with higher indexes. We are also using Einstein's 
    convention to sum over reapeated up and down indexes. So an inner product is always expressed as the sum of up and low indexes.
\end{remark}

Let us now ask: if $\bra{V}$ is the bra corresponding to the ket $\ket{V}$ what bra corresponds to $a\ket{V}$ where $a$ is some scalar?
By going to any basis it is readily found that
\begin{equation*}
    a\ket{V}\rightarrow
    \left[\begin{array}{c}
        av_1\\
        av_2\\
        \vdots\\
        av_n
    \end{array}\right]
    \rightarrow [a^*v_1^*, a^*v_2^*, \dots,a^*v_n^*]\rightarrow\bra{V}a^*.
\end{equation*}
It is customary to write $a\ket{V}$ as $\ket{aV}$ and the corresponding bra as $\bra{aV}$. What we have found is that
\begin{equation}
    \bra{aV}=\bra{V}a^*
\end{equation}

The inner product allows us to easily get the components of a vector in an orthonormal basis. Let $\ket{i}$ be an orthonormal basis, then
$\ket{V}=\sum v_i\ket{i}$.
Taking the inner product with a specific base vector $\ket{j}$ leads to $\scalar{j}{V}=v_j$ and therefore
\begin{equation}
    \ket{V}=\sum\ket{i}\scalar{i}{V}
\end{equation}

\begin{theorem}
    Any inner product satisfies two properties:
    \begin{enumerate}
        \item Schwarz inequality: $|\scalar{V}{W}|\leq|V||W|$.
        \item Triangle inequality: $|V+W|\leq|V|+|W|$.
    \end{enumerate}
    In both cases, the equality holds iif $\ket{V}$ and $\ket{W}$ are linearly dependent.
\end{theorem}
\begin{proof}
    For the Schwarz inequality, from
    \begin{equation*}
        \ket{Z}=\ket{V}-\frac{\scalar{W}{V}}{|W|^2}\ket{W}
    \end{equation*}
    we get
    \begin{equation*}
        0\leq\scalar{Z}{Z}=\scalar{V}{V}-\frac{\scalar{W}{V}\scalar{V}{W}}{|W|^2}
    \end{equation*}
    which leads to
    $|V|^2|W|^2\geq|\scalar{W}{V}|^2$ and therefore proves the Schwarz inequality.

    The proof for the triangle inequality is similar, starting with $|V+W|^2$ and considering $\mbox{Re}\scalar{V}{W}\leq|\scalar{V}{W}|$.
\end{proof}

\section{Linear operators}
An operator $\Omega$ is an instruction to transform a vector $\ket{V}$ into another vector $\ket{V'}$. This is represented by writing
$\Omega\ket{V}=\ket{V'}$.

\begin{definition}[Linear operator]
A \textit{linear operator} is an operator that satisfies
\begin{equation*}
    \Omega(\alpha\ket{V}+\beta\ket{W}) = \alpha\Omega\ket{V} + \beta\Omega\ket{W}
\end{equation*}
\end{definition}

\begin{remark}
    A nice feature of linear operators is that once their action on the basis vectors is known, their action on any vector in the space
    is determined. If we are given a basis $\ket{i}$ for our space, then the action of $\Omega$ on any vector $\ket{V}$ is given 
    by $\Omega\ket{V} = \sum v_i\Omega\ket{i}$
\end{remark}

The \textit{product of two operators} stands for the instruction that the action of the two operators be carried out in sequence.
Notice that the order of the operations matters, i.e., in general $\Omega\Lambda\neq\Lambda\Omega$. An example are rotations in 3D space.
\begin{definition}[Commutator]
    The commutator of two operators is defined as 
    \begin{equation*}
        [\Omega,\Lambda] = \Omega\Lambda-\Lambda\Omega
    \end{equation*}
\end{definition}

\begin{theorem}
    The commutator satisfies 
    \begin{equation}
        [\Omega, \Lambda\Theta] = \Lambda[\Omega, \Theta] + [\Omega, \Lambda]\Theta
    \end{equation}        
\end{theorem}
\begin{proof}
    Given a generic vector $\ket{V}$, we have
    \begin{equation*}
        [\Omega, \Lambda\Theta]\ket{V} = \Omega\Lambda\Theta\ket{V} - \Lambda\Theta\Omega\ket{V}
    \end{equation*}
Using the fact that $\Omega\Lambda = [\Omega,\Lambda]+\Lambda\Omega$ we can rewrite the previous equality as
\begin{equation*}
    [\Omega, \Lambda\Theta]\ket{V} = 
    [\Omega,\Lambda]\Theta\ket{V}+\Lambda\Omega\Theta\ket{V} - \Lambda\Theta\Omega\ket{V} =
    [\Omega,\Lambda]\Theta\ket{V} + \Lambda[\Omega,\Theta]\ket{V}
\end{equation*}
Since $\ket{V}$ is a generic vector, this equality must hold for all $\ket{V}\in V$.
\end{proof}

\begin{definition}
    The \textit{inverse} of $\Omega$, denoted by $\Omega^{-1}$, is defined such that $\Omega\Omega^{-1}=\Omega^{-1}\Omega=1$.
\end{definition}
\begin{remark}
    For linear operators acting on a vector space $V^n$ with $n$ finite, it can be shown that
    $\Omega\Omega^{-1}=1$ iif $\Omega^{-1}\Omega=1$.
\end{remark}
\begin{theorem}
    The inverse of a product of operators is given by
    \begin{equation}
        \left(\Omega\Lambda\right)^{-1} = \Lambda^{-1}\Omega^{-1}
    \end{equation}
\end{theorem}
\begin{proof}
    $\left(\Omega\Lambda\right)^{-1}$ is defined such that 
    \begin{equation*}
        \left(\Omega\Lambda\right)^{-1}\Omega\Lambda=1
    \end{equation*}
    If we multiply both sides by $\Lambda^{-1}$ to the right, we get 
    \begin{equation*}
        \left(\Omega\Lambda\right)^{-1}\Omega = \Lambda^{-1}
    \end{equation*}
    Multiplying both sides by $\Omega^{-1}$ to the right gives
    \begin{equation*}
        \left(\Omega\Lambda\right)^{-1} = \Lambda^{-1}\Omega^{-1}
    \end{equation*}
\end{proof}

\end{document}