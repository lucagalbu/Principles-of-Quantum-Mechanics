%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  My documentation report
%  Objetive: Explain what I did and how, so someone can continue with the investigation
%
% Important note:
% Chapter heading images should have a 2:1 width:height ratio,
% e.g. 920px width and 460px height.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[11pt,fleqn]{book} % Default font size and left-justified equations

\usepackage[top=3cm,bottom=3cm,left=3.2cm,right=3.2cm,headsep=10pt,letterpaper]{geometry} % Page margins

\usepackage{xcolor} % Required for specifying colors by name
\definecolor{ocre}{RGB}{52,177,201} % Define the orange color used for highlighting throughout the book

% Font Settings
\usepackage{avant} % Use the Avantgarde font for headings
%\usepackage{times} % Use the Times font for headings
\usepackage{mathptmx} % Use the Adobe Times Roman as the default text font together with math symbols from the Sym­bol, Chancery and Com­puter Modern fonts
\usepackage{microtype} % Slightly tweak font spacing for aesthetics
\usepackage[utf8]{inputenc} % Required for including letters with accents
\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage{amsthm}

% Bibliography
\usepackage{csquotes}
\usepackage[style=alphabetic,sorting=nyt,sortcites=true,autopunct=true,autolang=hyphen,hyperref=true,abbreviate=false,backref=true,backend=biber]{biblatex}
\addbibresource{bibliography.bib} % BibTeX bibliography file
\defbibheading{bibempty}{}

\input{structure} % Insert the commands.tex file which contains the majority of the structure behind the template

%----------------------------------------------------------------------------------------
%	Definitions of new commands
%----------------------------------------------------------------------------------------

\def\R{\mathbb{R}}
\newcommand{\cvx}{convex}
\newcommand{\bra}[1]{\langle #1|}
\newcommand{\ket}[1]{| #1\rangle}
\newcommand{\scalar}[2]{\langle #1| #2\rangle}
\begin{document}

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\begingroup
\thispagestyle{empty}
\AddToShipoutPicture*{\put(0,0){\includegraphics[scale=1.25]{esahubble}}} % Image background
\centering
\vspace*{5cm}
\par\normalfont\fontsize{35}{35}\sffamily\selectfont
\textbf{PRINCIPLES OF QUANTUM MECHANICS}\\
{\LARGE R. Shankar}\par % Book title
\vspace*{1cm}
{\Huge Lecture Notes}\par % Author name
\endgroup

%----------------------------------------------------------------------------------------
%	COPYRIGHT PAGE
%----------------------------------------------------------------------------------------

\newpage
~\vfill
\thispagestyle{empty}

%\noindent Copyright \copyright\ 2014 Andrea Hidalgo\\ % Copyright notice

\noindent \textsc{Notes from the book Principles of Quantum Mechanics.}
\vspace{\baselineskip}

\noindent These notes have been taking while self-studying the book Principles of Quantum Mechanics by R. Shankar, second edition.

\noindent \textit{First release, July 2022} % Printing/edition date

%----------------------------------------------------------------------------------------
%	TABLE OF CONTENTS
%----------------------------------------------------------------------------------------

\chapterimage{head1.png} % Table of contents heading image

\pagestyle{empty} % No headers

\tableofcontents % Print the table of contents itself

%\cleardoublepage % Forces the first chapter to start on an odd page so it's on the right

\pagestyle{fancy} % Print headers again

%----------------------------------------------------------------------------------------
%	CHAPTER 1
%----------------------------------------------------------------------------------------

\chapterimage{head2.png} % Chapter heading image
\chapter{Mathematical introduction}
\section{Vector spaces}
\begin{definition}[Group]
    A group is a set $G$ together with a binary operation here denoted "$\cdot$", that combines any two elements  $a$ and $b$ 
    to form an element of $G$, denoted $a\cdot b$, such that the following three requirements, known as group \textit{axioms}, 
    are satisfied
    \begin{enumerate}
        \item Associativity: $\forall\: a,b,c \in G\; (a\cdot b)\cdot c = a\cdot (b\cdot c)$.
        \item Identity: $\exists\: e \in G\;\mid\; \forall a\in G\; e\cdot a=a\cdot e=a$. 
        \item Inverse $\forall a\in G\;\exists\; a^{-1}\in G \mid a^{-1}\cdot a=a\cdot a^{-1}=e$.
    \end{enumerate}
\end{definition}

\begin{remark}
    In the definition of the identity and inverse element, we clearly stated that they commute with any element of the group. This is
    important to state in the defition, because the operation defined on the group is not necessarily commutative.    
\end{remark}

\begin{remark}
    From the assioms it follows that the identity element is unique. If $e$ and $f$ are both identity elements, then $e=e\cdot f=f$.
    
    Similarly, the inverse of an element $a$ of the group is also unique. If $a$ has both $a_1^{-1}$ and $a_2^{-1}$ as inverse, then
    $a_2^{-1} = a_2^{-1}\cdot e = a_2^{-1}\cdot(a\cdot a_1^{-1}) = (a_2^{-1}\cdot a)\cdot a_1^{-1}=e\cdot a_1^{-1}=a_1^{-1}$.
\end{remark}

\begin{definition}[Abelian group]
    A group is \textit{abelian} if the operation defined on it is commutative.
\end{definition}

\begin{definition}[Field]
    A field is a set $F$ together with two binary operations, called addition and multiplication; 
    it is an \textit{abelian} group under addition with $0$ as the additive identity; 
    the \textit{non zero} elements are an \textit{abelian} group under multiplication with $1$ as the multiplicative identity; 
    and multiplication distributes over addition: $\forall a,b\in F\; a\cdot(b+c)=(a\cdot b) + (a\cdot c)$.
\end{definition}

\begin{remark}
    $\R^2$ is a field with the multiplication defined by $(a,b)\cdot(c,d)=(ac-bd, ad+bc)$. This is exactly the multiplication of two
    complex numbers. Indeed, the complex numbers is the way to give $\R^2$ a field structure. For $n>2$ it is not possible to give $\R^n$
    the structure of a field. However, for $n=4$ we can have a pseudo field using the non-commutative quaternion multiplication.
    For $\R^8$ we can give up the associativity of the multiplication and get the non-associative Caley algebra (octanions).
\end{remark}

\begin{definition}[Vector space]
A vector space over a field $F$ is a set $V$ together with two binary operations. 
The first operation, called vector addition assigns to any two vectors $\ket{V}$ and $\ket{W}$ in $V$ a third vector in $V$ which is commonly written as $\ket{v+w}$.
The second operation, called scalar multiplication, assigns to any scalar $a$ in $F$ and any vector $\ket{V}$ in $V$ another vector in $V$, which is denoted $a\ket{V}$.
These two operations must satisfy the eight following axioms:
\begin{enumerate}
    \item Associativity of vector addition:
    $\forall\: \ket{U}, \ket{V}, \ket{W} \in V\; \ket{U}+(\ket{V}+\ket{W}) = (\ket{U}+\ket{V})+\ket{W}$.
    \item Commutativity of vector addition: $\forall\: \ket{U}, \ket{V} \in V\; \ket{U}+\ket{V}=\ket{V}+\ket{U}$.
    \item Identity element of vector addition:
    $\exists\: \ket{0}\in V\;\mid\;\forall \ket{V}\in V\;\ket{V} + \ket{0}= \ket{V}$. 
    \item Inverse elements of vector addition: $\forall \ket{V}\in V\;\exists\; \ket{-V}\in V \mid 
    \ket{V}+\ket{-V}=\ket{0}$.
    \item Compatibility of scalar multiplication with field multiplication:\\
    $\forall\: a,b\in F\: \ket{V}\in V\;a(b\ket{V}) = (ab)\ket{V}$.
    \item Identity element of scalar multiplication: if $1$ is the multiplicative identity in $F$, then $\forall\:\ket{V}\in V\;1\ket{V}=\ket{V}$.
    \item Distributivity of scalar multiplication with respect to vector addition: $\forall\: a\in F\:\ket{U},\ket{V}\in V\: a(\ket{U}+\ket{V})=a\ket{U}+a\ket{V}$.
    \item Distributivity of scalar multiplication with respect to field addition: 
    $\forall\: a,b\in F\:\ket{V}\in V\:(a+b)\ket{V} = a\ket{V}+b\ket{V}$.
\end{enumerate}
Subtraction of two vectors can be defined as $\ket{V}-\ket{W} = \ket{V} + \ket{-w}$.
\end{definition}

\begin{remark}
    From the axioms it follows that 
    \begin{enumerate}
        \item $\ket{0}$ and the inverse are unique. The proof is the same as for the case of the group above.
        \item $0\ket{V}=0$: $\ket{V}=1\ket{V}=(0+1)\ket{V}=0\ket{V}+\ket{V}\Rightarrow 0\ket{V}=0$.
        \item $\ket{-V}=-\ket{V}$
    \end{enumerate}
\end{remark}

\begin{example}
    Consider all functions $f(x)$ defined in an interval $0<x<L$ and such that $f(0)=f(L)=0$. 
    We define scalar multiplication by $a$ simply as $af(x)$ and addition as pointwise addition. These functions form a vector space where 
    $\ket{0}$ is the function which is zero everywhere and the additive inverse of $\ket{f(x)}$ is $\ket{-f(x)}$.
    Notice that if $f(0)=f(L)\neq 0$ then we do not have a vector space.
\end{example}

\subsection{Linear independency}
\begin{definition}[Linear independency]
    A set of vectors $\ket{i}$ is said to be linearly independent if
    $\sum a_i\ket{i}=\ket{0}$
    is satisfied only by the trivial solution $a_i=0$.
\end{definition}

\begin{remark}
    If a set of vectors is linearly dependent, there must exist at least two $a_i\neq 0$. Let's say $a_3\neq 0$, then
    $\ket{3}=-\sum_{i\neq 3}\frac{a_i}{a_3}\ket{i}$. That is, in a set of linearly dependent vectors, at least one of them
    can be written as a linear combination of the other. On the other hand, it is not possible to write any member of the
    linearly independent set in terms of the others.
\end{remark}

\begin{definition}[Dimension of the vector space]
    A vector space has dimension $n$ if it can accommodate a maximum
    of $n$ linearly independent vectors. It will be denoted by $V^n$.
\end{definition}

\begin{example}
    The set of $2 \times 2$ matrices is a four-dimensional vector space. In fact, the following four vectors are linearly indpenedent:
    \begin{equation*}
        \begin{array}{cccc}
            \ket{1}=\left[
                \begin{array}{cc}
                    1&0\\
                    0&0
                \end{array}
        \right] &
        \ket{2}=\left[
            \begin{array}{cc}
                0&1\\
                0&0
            \end{array}
        \right] &
        \ket{3}=\left[
            \begin{array}{cc}
                0&0\\
                1&0
            \end{array}
        \right] &
        \ket{4}=\left[
            \begin{array}{cc}
                0&0\\
                0&1
            \end{array}
        \right]
        \end{array}
    \end{equation*} 
    Notice that any other matrix can be written as a linear combination of these four matrices. We have seen that in a set of linearly
    independent vectors, no vector can be written as a linear combination of the others. This means that the four matrices given above 
    represent the largest set of linearly independent vectors, and the vector space of $2\times 2$ matrices has dimension $n=4$.
\end{example}

\begin{definition}[Basis]
    A set of $n$ linearly independent vectors in an $n$-dimensional space is called a \textit{basis}.
\end{definition}

\begin{definition}[Component]
    Given a basis $\ket{i}$, any vector $\ket{V}$ can be written as a linear combination of the vectors $\ket{i}$. The coefficients of the
    linear combination are called the \textit{components} of $\ket{V}$ in the basis $\ket{i}$.
\end{definition}

\begin{theorem}
    The expansion of a vector in a given basis is unique.
\end{theorem}
\begin{proof}
    Let's suppose that there exist two different expansions:
    \begin{equation*}
        \ket{V}=\sum a_i\ket{i} = \sum a'_i\ket{i}
    \end{equation*}
    It follows
    \begin{equation*}
        \sum a_i\ket{i} - \sum a'_i\ket{i} =  \sum (a_i-a'_i)\ket{i} = \ket{0}
    \end{equation*}
    Since $\ket{i}$ are linearly independent, this means that $a_i=a'_i$ and the expansion is unique.
\end{proof}

\subsection{Inner product space}
\begin{definition}[Inner product]
    The \textit{inner product} or \textit{scalar product} is an operation that takes two vectors $\ket{V}$ and $\ket{W}$ and
    returns a scalar, denoted $\scalar{W}{V}$ satisfying the following axioms:
    \begin{enumerate}
        \item Skew symmetry: $\scalar{W}{V}=\scalar{V}{W}^*$.
        \item Positive semidefiniteness: $\scalar{V}{V}\geq 0$ and the equality holds iff $\ket{V}=\ket{0}$.
        \item Linearity in the ket: $\bra{W}(a\ket{V}+b\ket{Z})=a\scalar{W}{V}+b\scalar{W}{Z}\equiv\scalar{W}{aV+bZ}$.
    \end{enumerate}
    A vector space together with an inner product is called \textit{inner product space}.
\end{definition}

\begin{remark}
    From the axioms of the inner product it follows:
    \begin{enumerate}
        \item $\scalar{V}{V}$ is a real non-negative number. Therefore, $\sqrt{\scalar{V}{V}}$ is well defined.
        \item Antilinearity with respect to the first factor: $\scalar{aW+bZ}{V}=a^*\scalar{W}{V}+b^*\scalar{Z}{V}$. 
    \end{enumerate}
\end{remark}

\begin{definition}[Norm and orthogonality]
    The \textit{norm} of a vector is defined as $|V|\equiv\sqrt{\scalar{V}{V}}$. 
    Two vectors $\ket{V}$ and $\ket{W}$ are \textit{orthogonal} if $\scalar{W}{V}=0$.
\end{definition}

\begin{remark}
    Orthogonal vectors are also linear independent. If $\ket{V}$ and $\ket{W}$ are orthogonal, let's consider
    $a\ket{V}+b\ket{W}=0$. Taking the inner product with $\ket{V}$ and using the orthogonality, we get $a=0$. Similarly, the
    inner product with $\ket{W}$ leads to $b=0$.
\end{remark}

\begin{definition}[Dual space and bras]
    Any ket $\ket{V}$ induces a linear function $\scalar{V}{\cdot}$ that associates to each ket $\ket{W}$ its inner product with 
    $\ket{V}$. The space of all linear functions $\scalar{V}{\cdot}$ generated by all vectors $\ket{V}$ of a vector space $V$ is
    is itself a vector space and is called the \textit{dual} of $V$. 
    The elements of the dual space are called \textit{bras} and denoted by $\bra{V}$.
\end{definition}

An inner product can take the form of any function satisfying the axioms. However, given an orthonormal basis, the inner product can be
expressed in an easy way in terms of the components of the vectors. Let $\ket{i}$ be an orthonormal basis, and let 
$\ket{V}=\sum v_i\ket{i}$ and $\ket{W}=\sum w_i\ket{i}$. Then, 
$\scalar{W}{V}=\sum_{i,j}w^*_iv_j\scalar{i}{j}=\sum_{i,j}w^*_iv_j\delta_{ij}=\sum_i w^*_iv_i$.
I.e., in an orthonormal basis, the inner product is just the sum of the products of the single components. However, which basis is 
orthonormal depends on the specific inner product chosen.

Notice that it is convention to write the components of a ket as a column vector.
So the inner product is a number we are trying to generate from two kets $\ket{V}$ and $\ket{W}$, which are both represented by column vectors
in some basis. Now there is no way to make a number out of two columns by direct matrix multiplication, but there is a way to make a number 
by matrix multiplication of a row times a column. In particular, if we represent the bra $\bra{W}$ with the \textit{adjoint} (transpose
conjugate) of $\ket{W}$, in an orthonormal basis we have 
\begin{equation}
    \label{eq:bra_components}
    \scalar{W}{V}=[w^*_1,...,w^*_n]
    \left[\begin{array}{c}
        v^1\\
        \vdots\\
        v^n
    \end{array}\right]=
    \sum_i w^*_iv^i\equiv w^*_iv^i
\end{equation}

\begin{remark}
    The components of the kets are written with lower indexes, while the components of a bra with higher indexes. We are also using Einstein's 
    convention to sum over reapeated up and down indexes. So an inner product is always expressed as the sum of up and low indexes.
\end{remark}

Let us now ask: if $\bra{V}$ is the bra corresponding to the ket $\ket{V}$ what bra corresponds to $a\ket{V}$ where $a$ is some scalar?
By going to any basis it is readily found that
\begin{equation*}
    a\ket{V}\rightarrow
    \left[\begin{array}{c}
        av_1\\
        av_2\\
        \vdots\\
        av_n
    \end{array}\right]
    \rightarrow [a^*v_1^*, a^*v_2^*, \dots,a^*v_n^*]\rightarrow\bra{V}a^*.
\end{equation*}
It is customary to write $a\ket{V}$ as $\ket{aV}$ and the corresponding bra as $\bra{aV}$. What we have found is that
\begin{equation}
    \label{eq:bra_components_2}
    \bra{aV}=\bra{V}a^*
\end{equation}

The inner product allows us to easily get the components of a vector in an orthonormal basis. Let $\ket{i}$ be an orthonormal basis, then
$\ket{V}=\sum v_i\ket{i}$.
Taking the inner product with a specific base vector $\ket{j}$ leads to $\scalar{j}{V}=v_j$ and therefore
\begin{equation}
    \ket{V}=\sum\ket{i}\scalar{i}{V}
\end{equation}

\begin{theorem}
    Any inner product satisfies two properties:
    \begin{enumerate}
        \item Schwarz inequality: $|\scalar{V}{W}|\leq|V||W|$.
        \item Triangle inequality: $|V+W|\leq|V|+|W|$.
    \end{enumerate}
    In both cases, the equality holds iif $\ket{V}$ and $\ket{W}$ are linearly dependent.
\end{theorem}
\begin{proof}
    For the Schwarz inequality, from
    \begin{equation*}
        \ket{Z}=\ket{V}-\frac{\scalar{W}{V}}{|W|^2}\ket{W}
    \end{equation*}
    we get
    \begin{equation*}
        0\leq\scalar{Z}{Z}=\scalar{V}{V}-\frac{\scalar{W}{V}\scalar{V}{W}}{|W|^2}
    \end{equation*}
    which leads to
    $|V|^2|W|^2\geq|\scalar{W}{V}|^2$ and therefore proves the Schwarz inequality.

    The proof for the triangle inequality is similar, starting with $|V+W|^2$ and considering $\mbox{Re}\scalar{V}{W}\leq|\scalar{V}{W}|$.
\end{proof}

\section{Linear operators}
An operator $\Omega$ is an instruction to transform a vector $\ket{V}$ into another vector $\ket{V'}$. This is represented by writing
$\Omega\ket{V}=\ket{V'}$.

\begin{definition}[Linear operator]
A \textit{linear operator} is an operator that satisfies
\begin{equation*}
    \Omega(\alpha\ket{V}+\beta\ket{W}) = \alpha\Omega\ket{V} + \beta\Omega\ket{W}
\end{equation*}
\end{definition}

\begin{remark}
    A nice feature of linear operators is that once their action on the basis vectors is known, their action on any vector in the space
    is determined. If we are given a basis $\ket{i}$ for our space, then the action of $\Omega$ on any vector $\ket{V}$ is given 
    by $\Omega\ket{V} = \sum v_i\Omega\ket{i}$
\end{remark}

The \textit{product of two operators} stands for the instruction that the action of the two operators be carried out in sequence.
Notice that the order of the operations matters, i.e., in general $\Omega\Lambda\neq\Lambda\Omega$. An example are rotations in 3D space.
\begin{definition}[Commutator]
    The commutator of two operators is defined as 
    \begin{equation*}
        [\Omega,\Lambda] = \Omega\Lambda-\Lambda\Omega
    \end{equation*}
\end{definition}

\begin{theorem}
    The commutator satisfies 
    \begin{equation}
        [\Omega, \Lambda\Theta] = \Lambda[\Omega, \Theta] + [\Omega, \Lambda]\Theta
    \end{equation}        
\end{theorem}
\begin{proof}
    Given a generic vector $\ket{V}$, we have
    \begin{equation*}
        [\Omega, \Lambda\Theta]\ket{V} = \Omega\Lambda\Theta\ket{V} - \Lambda\Theta\Omega\ket{V}
    \end{equation*}
Using the fact that $\Omega\Lambda = [\Omega,\Lambda]+\Lambda\Omega$ we can rewrite the previous equality as
\begin{equation*}
    [\Omega, \Lambda\Theta]\ket{V} = 
    [\Omega,\Lambda]\Theta\ket{V}+\Lambda\Omega\Theta\ket{V} - \Lambda\Theta\Omega\ket{V} =
    [\Omega,\Lambda]\Theta\ket{V} + \Lambda[\Omega,\Theta]\ket{V}
\end{equation*}
Since $\ket{V}$ is a generic vector, this equality must hold for all $\ket{V}\in V$.
\end{proof}

\begin{definition}
    The \textit{inverse} of $\Omega$, denoted by $\Omega^{-1}$, is defined such that $\Omega\Omega^{-1}=\Omega^{-1}\Omega=1$.
\end{definition}
\begin{remark}
    For linear operators acting on a vector space $V^n$ with $n$ finite, it can be shown that
    $\Omega\Omega^{-1}=1$ iif $\Omega^{-1}\Omega=1$.
\end{remark}
\begin{theorem}
    The inverse of a product of operators is given by
    \begin{equation}
        \left(\Omega\Lambda\right)^{-1} = \Lambda^{-1}\Omega^{-1}
    \end{equation}
\end{theorem}
\begin{proof}
    $\left(\Omega\Lambda\right)^{-1}$ is defined such that 
    \begin{equation*}
        \left(\Omega\Lambda\right)^{-1}\Omega\Lambda=1
    \end{equation*}
    If we multiply both sides by $\Lambda^{-1}$ to the right, we get 
    \begin{equation*}
        \left(\Omega\Lambda\right)^{-1}\Omega = \Lambda^{-1}
    \end{equation*}
    Multiplying both sides by $\Omega^{-1}$ to the right gives
    \begin{equation*}
        \left(\Omega\Lambda\right)^{-1} = \Lambda^{-1}\Omega^{-1}
    \end{equation*}
\end{proof}

\subsection{Matrix elements}
We have seen that an abstract vector can be represented in a basis by an n-tuple of numbers, called its components,
in terms of which all vector operations can be carried out.
We shall now see that in the same manner a linear operator can be represented in a basis by an $n\times n$
matrix.

Let's consider $\ket{V'}=\Omega\ket{V}$. In terms of the components on basis $\ket{i}$, this can be rewritten
\begin{equation*}
    \sum v'_i\ket{i} = \sum v_i\Omega\ket{i}
\end{equation*}
Taking the inner product with $\ket{j}$ of both sides gives an expression for $v'_j$
\begin{equation*}
    v'_j = v_j\bra{j}\Omega\ket{i}
\end{equation*}
And therefore
\begin{equation*}
    \sum v'_i\ket{i} = \sum v_j\bra{j}\Omega\ket{i}
\end{equation*}
If we define the matrix
\begin{equation}
    \label{eq:matrix_operator}
    \Omega^j\:_i\equiv \bra{j}\Omega\ket{i}
\end{equation}
We can write the transformed vector $\ket{V'}=\Omega\ket{V}$ as the matrix multiplication
\begin{equation}
    (v')^j=\Omega^j\:_iv^i    
\end{equation}
where we used the Einstein's sum convention.

\begin{remark}
    A mnemonic: the elements of the $i$-th column of $\Omega^j\:_i$ are simply the components of the $i$-th
transformed basis vector $\ket{i'}=\Omega\ket{i}$ in the given basis.
\end{remark}

\begin{example}
    Let's consider rotations in 3D space, i.e. the operator $R\left(\frac{1}{2}\pi \hat i\right)$ that rotates
    a vector by $\frac{1}{2}\pi$ about the unit vector $\hat i$. Notice that this operator is linear.
    
    Let us consider the action of this operator on the three unit vectors $\hat i$, $\hat j$, and $\hat k$, which
    in our notation will be denoted by $\ket{1}$, $\ket{2}$, $\ket{3}$
    \begin{eqnarray*}
        \left(\frac{1}{2}\pi \hat i\right)\ket{1} &=& \ket{1} \\
        \left(\frac{1}{2}\pi \hat i\right)\ket{2} &=& \ket{3} \\
        \left(\frac{1}{2}\pi \hat i\right)\ket{3} &=& -\ket{2} \\
    \end{eqnarray*}

    Following our mnemonic, the matrix representation of $R\left(\frac{1}{2}\pi \hat i\right)$ is
    \begin{equation*}
        R\left(\frac{1}{2}\pi \hat i\right) \leftrightarrow \left[\begin{array}{ccc}
            1&0&0\\
            0&0&-1\\
            0&1&0
        \end{array}\right]
    \end{equation*}
\end{example}

\subsubsection{Projection operator}
We saw that a vector $\ket{V}$ can be expanded on a basis with the formula $\ket{V}=\sum\ket{i}\bra{i}\ket{V}$.
Each single term of the expansions, $\ket{V}=\ket{i}\bra{i}\ket{V}$ can be interpreted as a sort of projection of the 
vector onto the $i$-th basis vector. And this leads to the following definition
\begin{definition}[Projection operator]
    The operator $P_i = \ket{i}\bra{i}$ is a linear operator called the \textit{projection operator} for the ket $\ket{i}$.
\end{definition}
\begin{theorem}[Completeness]
    The projection operator satisfies the completeness relation, i.e., 
    \begin{equation}
        \label{eq:completness}
        \sum P_i =\sum\ket{i}\bra{i }= 1
    \end{equation}
\end{theorem}
\begin{proof}
    A generic vector $\ket{V}$ can be expanded on a basis using the projection operators
    \begin{equation*}
        \ket{V} = \sum P_i\ket{V} = \left(\sum P_i\right)\ket{V} 
    \end{equation*}
    Since $\ket{V}$ is a generic vector, it must hold that $\left(\sum P_i\right)=1$.
\end{proof}
\begin{remark}
    The completeness relation says that the sum of the projections of a vector along all the $n$
    directions equals the vector itself.
\end{remark}
\begin{remark}
    Pojection operators corresponding to the basis vectors obey $P_jP_i=\delta_{ij}$.
    This tells us that once $P_i$ projects out the part of $\ket{V}$ along $\ket{i}$, further applications of $P_i$, make
    no difference; and the subsequent application of $P_j$ with $j\neq i$ will result in zero, since a vector entirely
    along $\ket{i}$ cannot have a projection along a perpendicular direction $\ket{j}$.
\end{remark}

The matrix elements of $P_i$ are given by Eq. \ref{eq:matrix_operator}
\begin{equation*}
    \left(P_i\right)^k\:_l = \bra{k}P_i\ket{l} = \scalar{k}{l}\delta_{il} = \delta_{kl}\delta_{il} = \delta^k\:_i\delta_{il}
\end{equation*}
This matrix has $0$ everywhere, except for $1$ at position $(i,i)$. This is in agreement with our mnemonic that the matrix of a linear
operator contains per columns the components of the transformed basis vectors. In the case of the projection operator $P_i$, all base 
vectors except $\ket{i}$ gives $\ket{0}$. But the base vector $\ket{i}$  stays untrasformed.

The matrix also arises if we consider $\ket{i}\bra{i}$ component-wise. $\ket{i}$ is a column vector of length $n$, while $\bra{i}$ 
is a row vector of dimension $n$. Therefore, $\ket{i}\bra{i}$ is the multiplication of a $1\times n$ matrix with a $n\times 1$ matrix,
which gives the $n\times n$ matrix reported above.

\begin{remark}
    The completeness relation, Eq. \ref{eq:completness} says that when all the $P_i$ are added, the diagonal fills out to give the
    identity. If we form the sum over just some of the projection operators, we get the operator which projects a given vector into the 
    subspace spanned by just the corresponding basis vectors.
\end{remark}

\subsection{Adjoint of an operator}
We have seen the action of an operator on a ket $\ket{V}$. But what about the action on a bra $\bra{V}$?
In a given basis, we have seen that the bra is the transpose conjugate of the corresponding ket (see Eq. \ref{eq:bra_components}
and Eq. \ref{eq:bra_components_2}). Here will see that the same applies for operators.

\begin{definition}[Adjoint]
    If $\ket{\Omega}$ is the operator that turns a ket $\ket{V}$ into $\ket{V'}$, then its \textit{adjoint} $\Omega^\dagger$ is 
    defined as the operator that turns the bra $\bra{V}$ into $\bra{V'}$.
\end{definition}
\begin{theorem}
    In a given basis, the matrix representing $\Omega^\dagger$ is given by the transpose conjugate of the matrix
    representing $\Omega$.
\end{theorem}
\begin{proof}
    \begin{equation*}
        \left(\Omega^\dagger\right)_i\:^j = \bra{i}\Omega^\dagger\ket{j} = \scalar{\Omega i}{j} = \scalar{j}{\Omega i}^* = 
        \bra{j}\Omega\ket{i}^* = \left(\Omega^*\right)^j\:_i
    \end{equation*}
\end{proof}

\begin{theorem}
    The adjoint of a product is the product of the adjoints in reverse:
    \begin{equation}
        \left(\Omega\Lambda\right)^\dagger=\Lambda^\dagger\Omega^\dagger
    \end{equation}
\end{theorem}
\begin{proof}
    Let's consider $\bra{\Omega\Lambda V}$. First we treat $\Omega\Lambda$ as one operator and get
    \begin{equation*}
        \bra{\Omega\Lambda V} = \bra{V}\left(\Omega\Lambda\right)^\dagger
    \end{equation*}
    Next we treat $\Lambda$ and $\Omega$ as two distinct operators applied sequentially
    \begin{equation*}
        \bra{\Omega\Lambda V} = \bra{\Lambda V}\Omega^\dagger = \bra{V}\Lambda^\dagger\Omega^\dagger
    \end{equation*}
    Comparing the two equations and considering that $\ket{V}$ is a generic ket,
    we get the desired result $\left(\Omega\Lambda\right)^\dagger=\Lambda^\dagger\Omega^\dagger$.
\end{proof}


When a product of operators, bras, kets, and explicit numerical coefficients is encountered, reverse
the order of all factors and make the substitutions
\begin{eqnarray}
    \nonumber
    \Omega&\leftrightarrow&\Omega^\dagger
    \\
    \ket{V}&\leftrightarrow&\bra{V}
    \\\nonumber
    a&\leftrightarrow&a^*
\end{eqnarray}

\begin{example}
    Let's consider the following equation
    \begin{equation*}
        a_1\ket{V_1} = a_2\ket{V_2} + a_3\ket{V_3}\scalar{V_4}{V_5}+ a_4\Omega\Lambda\ket{V_6} 
    \end{equation*}
    Its adjoint is
    \begin{equation*}
        \bra{V_1}a^*_1 = \bra{V_2}a^*_2+ \scalar{V_4}{V_5}\bra{V_3}a^*_3+ \bra{V_6}\Lambda^\dagger\Omega^\dagger a_4^*
    \end{equation*}
    Of course, there is no real need to reverse the location of the scalars $a$ except in the interest of uniformity.
\end{example}

\begin{definition}[Hermitian and unitary  operators]
    An operator is called 
    \begin{enumerate}
        \item \textit{Hermitian} if $\Omega^\dagger=\Omega$.
        \item \textit{Anti-Hermitian} if $\Omega^\dagger=-\Omega$.
        \item \textit{Unitary} if $\Omega\Omega^\dagger=1$.
    \end{enumerate}
\end{definition}

\begin{remark}
    Hermitian and anti-Hermitian operators are the equivalent of pure real and pure imaginary complex numbers. Unitary operators
    are the equivalent of complex numbers of unit modulus $e^{i\theta}$.
\end{remark}

\begin{theorem}
    Unitary operators preserve the inner product between the vectors they act on.
\end{theorem}
\begin{proof}
    Let $\ket{V'_1}=U\ket{V_1}$ and $\ket{V'_2}=U\ket{V_2}$. Then 
    \begin{equation*}
        \scalar{V'_1}{V'_2} = \bra{V_1}U^\dagger U\ket{V_2} = \scalar{V_1}{V_2}
    \end{equation*}
\end{proof}

\begin{remark}
    Unitary operators are the generalizations of rotation operators from $V^3(\mathbb{R})$ to $V^3(\mathbb{C})$,
    for just like rotation operators in three dimensions, they preserve the lengths of vectors and their dot products. 
    In fact, on a real vector space, the unitarity condition becomes $U^\dagger=U^T$, which defines an orthogonal or
    rotation matrix.
\end{remark}

\subsubsection{Active and passive transformations}
Suppose we subject all the vectors in a space to a unitary transformation $\ket{V}\rightarrow U\ket{V}$.
The matrix elements of any operator $\Omega$ are transformed as 
\begin{equation*}
    \bra{W}\Omega\ket{V}\rightarrow \bra{UW}\Omega\ket{UV}=\bra{W}U^\dagger\Omega U\ket{V}
\end{equation*}
It is clear that we would obtain the same matrix if we left the vectors alone and subjected all operators
to the change
\begin{equation*}
    \Omega\rightarrow U^\dagger\Omega U
\end{equation*}
The first case is called \textit{active transformation}, while the second one \textit{passive transformation}.
Later we will see that the physics in quantum theory lies in the matrix elements of operators, and that active and
passive transformations provide us with two equivalent ways of describing the same physical transformation.
\end{document}